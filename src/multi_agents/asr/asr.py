#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @File  : asr.py
# @Author: Qiyu (Allen) Zhong
# @Date  : 2024/10/3
# @Desc  :

import os
import gradio as gr
from typing import List, Iterable
from loguru import logger
from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler
from sparkai.core.messages import ChatMessage

class Config:
    def __init__(self, appid: str = None, apikey: str = None, apisecret: str = None):
        """
        åˆå§‹åŒ–è®¯é£APIçš„ç¯å¢ƒé…ç½®
        :param appid: è®¯é£APIçš„App ID
        :param apikey: è®¯é£APIçš„API Key
        :param apisecret: è®¯é£APIçš„API Secret
        """
        self.XF_APPID = appid or os.environ.get("SPARKAI_APP_ID")
        self.XF_APIKEY = apikey or os.environ.get("SPARKAI_API_KEY")
        self.XF_APISECRET = apisecret or os.environ.get("SPARKAI_API_SECRET")

class ChatModel:
    def __init__(self, config: Config, domain: str = 'generalv3.5', model_url: str = 'wss://spark-api.xf-yun.com/v3.5/chat', stream: bool = False):
        """
        åˆå§‹åŒ–èŠå¤©æ¨¡å‹
        :param config: é¡¹ç›®é…ç½®æ–‡ä»¶
        :param domain: æ¨¡å‹åŸŸå
        :param model_url: æ¨¡å‹åœ°å€
        :param stream: æ˜¯å¦å¯ç”¨æµå¼è°ƒç”¨
        """
        self.spark = ChatSparkLLM(
            spark_api_url=model_url,
            spark_app_id=config.XF_APPID,
            spark_api_key=config.XF_APIKEY,
            spark_api_secret=config.XF_APISECRET,
            spark_llm_domain=domain,
            streaming=stream,
        )
        self.stream = stream

    def generate(self, msgs: str | List[ChatMessage]) -> str:
        """
        æ‰¹é‡ç”Ÿæˆå¯¹è¯
        :param msgs: æ¶ˆæ¯åˆ—è¡¨
        :return: ç”Ÿæˆçš„å¯¹è¯æ–‡æœ¬
        """
        if self.stream:
            raise Exception('æ¨¡å‹åˆå§‹åŒ–ä¸ºæµå¼è¾“å‡ºï¼Œè¯·è°ƒç”¨generate_streamæ–¹æ³•')

        messages = self.__trans_msgs(msgs)
        resp = self.spark.generate([messages])
        return resp.generations[0][0].text

    def generate_stream(self, msgs: str | List[ChatMessage]) -> Iterable[str]:
        """
        æµå¼ç”Ÿæˆå¯¹è¯
        :param msgs: æ¶ˆæ¯åˆ—è¡¨
        :return: ç”Ÿæˆçš„å¯¹è¯æ–‡æœ¬æµ
        """
        if not self.stream:
            raise Exception('æ¨¡å‹åˆå§‹åŒ–ä¸ºæ‰¹å¼è¾“å‡ºï¼Œè¯·è°ƒç”¨generateæ–¹æ³•')
        messages = self.__trans_msgs(msgs)
        resp_iterable = self.spark.stream(messages)
        for resp in resp_iterable:
            yield resp.content

    def __trans_msgs(self, msg: str):
        """
        å†…éƒ¨æ–¹æ³•ï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ¶ˆæ¯å¯¹è±¡
        :param msg: å­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨
        :return: æ¶ˆæ¯åˆ—è¡¨
        """
        if isinstance(msg, str):
            return [ChatMessage(role="user", content=msg)]
        return msg

class SparkApp:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–åº”ç”¨ç¨‹åº
        :param config: é…ç½®æ–‡ä»¶å¯¹è±¡
        """
        self.config = config
        self.model = ChatModel(config)
    
    def chat_interface(self):
        """
        å®šä¹‰èŠå¤©ç•Œé¢
        """
        with gr.Blocks() as demo:
            chatbot = gr.Chatbot([], elem_id="chat-box", label="èŠå¤©å†å²")
            chat_query = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="è¾“å…¥éœ€è¦å’¨è¯¢çš„é—®é¢˜")
            llm_submit_tab = gr.Button("å‘é€", visible=True)
            gr.Examples(["è¯·ä»‹ç»ä¸€ä¸‹Datawhaleã€‚", "å¦‚ä½•åœ¨å¤§æ¨¡å‹åº”ç”¨æ¯”èµ›ä¸­çªå›´å¹¶è·å¥–ï¼Ÿ", "è¯·ä»‹ç»ä¸€ä¸‹åŸºäºGradioçš„åº”ç”¨å¼€å‘"], chat_query)
            chat_query.submit(fn=self.chat, inputs=[chat_query, chatbot], outputs=[chat_query, chatbot])
            llm_submit_tab.click(fn=self.chat, inputs=[chat_query, chatbot], outputs=[chat_query, chatbot])

        demo.queue().launch()

    def chat(self, chat_query, chat_history):
        """
        å¤„ç†èŠå¤©è¯·æ±‚
        :param chat_query: ç”¨æˆ·è¾“å…¥çš„èŠå¤©å†…å®¹
        :param chat_history: èŠå¤©å†å²è®°å½•
        :return: æ›´æ–°åçš„èŠå¤©å†å²è®°å½•
        """
        bot_message = self.model.generate(chat_query)
        chat_history.append((chat_query, bot_message))
        return "", chat_history

    def run_text_to_audio(self, text: str, audio_path: str):
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³
        :param text: è¾“å…¥çš„æ–‡æœ¬
        :param audio_path: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        t2a = Text2Audio(self.config)
        t2a.gen_audio(text, audio_path)

    def run_audio_to_text(self, audio_path: str):
        """
        å°†è¯­éŸ³è½¬æ¢ä¸ºæ–‡æœ¬
        :param audio_path: è¾“å…¥çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :return: è½¬æ¢åçš„æ–‡æœ¬
        """
        a2t = Audio2Text(self.config)
        audio_text = a2t.gen_text(audio_path)
        return audio_text

    def run_text_to_img(self, prompt: str, img_path: str):
        """
        æ ¹æ®æ–‡æœ¬ç”Ÿæˆå›¾ç‰‡
        :param prompt: è¾“å…¥çš„æç¤ºæ–‡æœ¬
        :param img_path: ç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        """
        t2i = Text2Img(self.config)
        t2i.gen_image(prompt, img_path)

    def run_image_understanding(self, prompt: str, img_path: str):
        """
        å›¾ç‰‡ç†è§£
        :param prompt: è¾“å…¥çš„æç¤ºæ–‡æœ¬
        :param img_path: è¾“å…¥çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        :return: å›¾ç‰‡ç†è§£ç»“æœ
        """
        iu = ImageUnderstanding(self.config)
        return iu.understanding(prompt, img_path)

    def run_get_embedding(self, text: str):
        """
        è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
        :param text: è¾“å…¥çš„æ–‡æœ¬
        :return: æ–‡æœ¬çš„åµŒå…¥å‘é‡
        """
        em = EmbeddingModel(self.config)
        return em.get_embedding(text)

    def save_prompts(self, ask_batch, answer_batch, batch_related_classes, language='en'):
        """
        å°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®å’Œç›¸å…³ä¿¡æ¯ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
        :param ask_batch: è¾“å…¥é—®é¢˜çš„æ‰¹æ¬¡
        :param answer_batch: è¾“å‡ºç­”æ¡ˆçš„æ‰¹æ¬¡
        :param batch_related_classes: ç›¸å…³ç±»åˆ«
        :param language: å¤„ç†çš„è¯­è¨€
        """
        file_name = f'prompts_{language}.txt'
        with open(file_name, 'a', encoding='utf-8') as f:
            for ask, answer, label in zip(ask_batch, answer_batch, batch_related_classes):
                f.write(f"{ask}\t{answer}\t{label}\n")

if __name__ == '__main__':
    # åˆå§‹åŒ–é…ç½®
    config = Config()
    # åˆ›å»ºåº”ç”¨
    app = SparkApp(config)
    # å¯åŠ¨èŠå¤©ç•Œé¢
    app.chat_interface()

# denoise.py

import os
import subprocess
import glob

# å®šä¹‰è¾“å…¥ã€ä¸­é—´å’Œè¾“å‡ºæ–‡ä»¶å¤¹
input_folder = r"C:\Users\xiaoy\Downloads\wav"
intermediate_folder = r"C:\Users\xiaoy\Downloads\pcm"
output_folder = r"C:\Users\xiaoy\Downloads\m4a"

# ç¡®ä¿ä¸­é—´å’Œè¾“å‡ºæ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(intermediate_folder, exist_ok=True)
os.makedirs(output_folder, exist_ok=True)

# æ£€æŸ¥æ˜¯å¦æœ‰éŸ³é¢‘æ–‡ä»¶
audio_files = glob.glob(os.path.join(input_folder, '*.[mM][4aA]')) + glob.glob(os.path.join(input_folder, '*.mp3'))

if len(audio_files) == 0:
    print(f"No audio files found in {input_folder}")
    exit(1)

# ç»å¯¹è·¯å¾„çš„ rnnoise_demo
rnnoise_path = r"/absolute/path/to/rnnoise_demo"

for input_file in audio_files:
    # æå–æ–‡ä»¶æ‰©å±•åå’ŒåŸºæœ¬åç§°
    base_name, extension = os.path.splitext(os.path.basename(input_file))
    extension = extension.lower().strip('.')

    print(f"Processing {input_file} with extension {extension}")

    # è½¬æ¢åˆ° PCM æ ¼å¼
    pcm_path = os.path.join(intermediate_folder, f"{base_name}.pcm")
    try:
        subprocess.run(['ffmpeg', '-i', input_file, '-f', 's16le', '-acodec', 'pcm_s16le', pcm_path], check=True)
        print(f"Converted to PCM: {pcm_path}")
    except subprocess.CalledProcessError:
        print(f"Failed to convert {input_file} to PCM")
        continue

    # åº”ç”¨ rnnoise é™å™ª
    denoised_pcm_path = os.path.join(intermediate_folder, f"{base_name}_denoised.pcm")
    if os.path.isfile(pcm_path):
        subprocess.run([rnnoise_path, pcm_path, denoised_pcm_path], check=True)
        print(f"Noise reduction applied: {denoised_pcm_path}")

    # å°†é™å™ªåçš„ PCM æ–‡ä»¶è½¬æ¢å› M4A æ ¼å¼
    if os.path.isfile(denoised_pcm_path):
        output_path = os.path.join(output_folder, f"{base_name}_denoised.m4a")
        subprocess.run(['ffmpeg', '-f', 's16le', '-ar', '44100', '-ac', '1', '-i', denoised_pcm_path, output_path], check=True)
        print(f"Converted {denoised_pcm_path} to {output_path} as .m4a")

    # å¦‚æœåŸæ–‡ä»¶æ˜¯ MP3ï¼Œç›´æ¥è½¬æ¢ä¸º M4A
    if extension == "mp3":
        direct_output_path = os.path.join(output_folder, f"{base_name}.m4a")
        subprocess.run(['ffmpeg', '-i', input_file, direct_output_path], check=True)
        print(f"Directly converted {input_file} to {direct_output_path} as .m4a")

    # åˆ é™¤ä¸­é—´æ–‡ä»¶
    if os.path.isfile(pcm_path):
        os.remove(pcm_path)
    if os.path.isfile(denoised_pcm_path):
        os.remove(denoised_pcm_path)

print(f"All files processed and saved to {output_folder}")

# æ¸…ç†æ®‹ç•™çš„ä¸­é—´æ–‡ä»¶
for pcm_file in glob.glob(os.path.join(intermediate_folder, '*.pcm')):
    os.remove(pcm_file)

# app.py

import os
import gradio as gr
import random
import time
from sparkai.core.messages import ChatMessage
from dwspark.config import Config
from dwspark.models import ChatModel, Text2Img, ImageUnderstanding, Text2Audio, Audio2Text, EmebddingModel
from loguru import logger

# åŠ è½½è®¯é£çš„apié…ç½®
SPARKAI_APP_ID = os.environ.get("SPARKAI_APP_ID", "your_app_id")
SPARKAI_API_SECRET = os.environ.get("SPARKAI_API_SECRET", "your_api_secret")
SPARKAI_API_KEY = os.environ.get("SPARKAI_API_KEY", "your_api_key")
config = Config(SPARKAI_APP_ID, SPARKAI_API_KEY, SPARKAI_API_SECRET)

# åˆå§‹åŒ–æ¨¡å‹
stream_model = ChatModel(config, stream=True)

# ä¸­è¯‘è‹±æç¤ºè¯­
zh2en_prompt = 'ä½ æ˜¯ä¸­è‹±æ–‡äº’è¯‘é«˜æ‰‹ã€‚ç»™å®šä¸€å¥ä¸­æ–‡æ–‡æœ¬ï¼Œè¯·ä½ å¸®æˆ‘ç¿»è¯‘æˆè‹±æ–‡ã€‚æ–‡æœ¬ï¼š{}'
# è‹±è¯‘ä¸­æç¤ºè¯­
en2zh_prompt = 'ä½ æ˜¯ä¸­è‹±æ–‡äº’è¯‘é«˜æ‰‹ã€‚ç»™å®šä¸€å¥è‹±æ–‡æ–‡æœ¬ï¼Œè¯·ä½ å¸®æˆ‘ç¿»è¯‘æˆä¸­æ–‡ã€‚æ–‡æœ¬ï¼š{}'

def chat(chat_query, chat_history, prompt_type):
    if prompt_type == 'ä¸­è¯‘è‹±':
        final_query = zh2en_prompt.format(chat_query)
    else:
        final_query = en2zh_prompt.format(chat_query)
    # æ·»åŠ æœ€æ–°é—®é¢˜
    prompts = [ChatMessage(role='user', content=final_query)]

    # å°†é—®é¢˜è®¾ä¸ºå†å²å¯¹è¯
    chat_history.append((chat_query, ''))
    # å¯¹è¯åŒæ—¶æµå¼è¿”å›
    for chunk_text in stream_model.generate_stream(prompts):
        # æ€»ç»“ç­”æ¡ˆ
        answer = chat_history[-1][1] + chunk_text
        # æ›¿æ¢æœ€æ–°çš„å¯¹è¯å†…å®¹
        chat_history[-1] = (chat_query, answer)
        # è¿”å›
        yield '', chat_history

# éšæœºèŠå¤©å‡½æ•°
def random_chat(chat_query, chat_history):
    bot_message = random.choice(["How are you?", "I love you", "I'm very hungry"])
    chat_history.append((chat_query, bot_message))
    return "", chat_history

with gr.Blocks() as demo:
    warning_html_code = """
        <div class="hint" style="text-align: center;background-color: rgba(255, 255, 0, 0.15); padding: 10px; margin: 10px; border-radius: 5px; border: 1px solid #ffcc00;">
            <p>ä¸­è‹±ç¿»è¯‘åŠ©æ‰‹æ˜¯Datawhaleå¼€æºã€Šè®¯é£2024æ˜Ÿç«æ¯ã€‹ç¬¬ä¸€é˜¶æ®µçš„baselineã€‚</p>
            <p>ğŸ± æ¬¢è¿ä½“éªŒæˆ–äº¤æµã€å…¬ä¼—å·ã€‘ï¼šDatawhale ã€Bç«™ä¸»é¡µã€‘https://space.bilibili.com/431850986</p>
            <p>ç›¸å…³åœ°å€: <a href="https://challenge.xfyun.cn/h5/xinghuo?ch=dwm618">æ¯”èµ›åœ°å€</a>ã€<a href="https://datawhaler.feishu.cn/wiki/Aee0wU4KlipwY9kHJyecQFT3nTg">å­¦ä¹ æ‰‹å†Œ</a></p>
        </div>
    """
    gr.HTML(warning_html_code)

    prompt_type = gr.Radio(choices=['ä¸­è¯‘è‹±', 'è‹±è¯‘ä¸­'], value='ä¸­è¯‘è‹±', label='ç¿»è¯‘ç±»å‹')
    chatbot = gr.Chatbot([], elem_id="chat-box", label="èŠå¤©å†å²")
    chat_query = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="è¾“å…¥éœ€è¦å’¨è¯¢çš„é—®é¢˜")
    llm_submit_tab = gr.Button("å‘é€", visible=True)
    gr.Examples([
        "Datawhale æ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•°æ®ç§‘å­¦ä¸ AI é¢†åŸŸçš„å¼€æºç»„ç»‡...",
        "Python is a programming language that lets you work quickly and integrate systems more effectively."
    ], chat_query)

    # æŒ‰é’®è§¦å‘é€»è¾‘
    llm_submit_tab.click(fn=chat, inputs=[chat_query, chatbot, prompt_type], outputs=[chat_query, chatbot])
    chat_query.submit(fn=random_chat, inputs=[chat_query, chatbot], outputs=[chat_query, chatbot])

    # æ·»åŠ  SDK åŠŸèƒ½ç¤ºä¾‹
    with gr.Accordion("SDK åŠŸèƒ½ç¤ºä¾‹", open=False):
        with gr.Column():
            text_to_audio = gr.Button("æ–‡å­—ç”Ÿæˆè¯­éŸ³")
            audio_to_text = gr.Button("è¯­éŸ³è¯†åˆ«æ–‡å­—")
            generate_image = gr.Button("ç”Ÿæˆå›¾ç‰‡")
            understand_image = gr.Button("å›¾ç‰‡è§£é‡Š")
            get_embedding = gr.Button("è·å–æ–‡æœ¬å‘é‡")
            
            # SDK ç¤ºä¾‹åŠŸèƒ½
            def t2a_function():
                text = '2023å¹´5æœˆï¼Œè®¯é£æ˜Ÿç«å¤§æ¨¡å‹æ­£å¼å‘å¸ƒ...'
                audio_path = './demo.mp3'
                t2a = Text2Audio(config)
                t2a.gen_audio(text, audio_path)
                return f"éŸ³é¢‘å·²ç”Ÿæˆ: {audio_path}"

            def a2t_function():
                a2t = Audio2Text(config)
                audio_text = a2t.gen_text('./demo.mp3')
                return audio_text

            def t2i_function():
                prompt = 'ä¸€åªé²¸é±¼åœ¨å¿«ä¹æ¸¸æ³³çš„å¡é€šå¤´åƒ'
                t2i = Text2Img(config)
                t2i.gen_image(prompt, './demo.jpg')
                return './demo.jpg'

            def iu_function():
                iu = ImageUnderstanding(config)
                understanding = iu.understanding('è¯·ç†è§£ä¸€ä¸‹å›¾ç‰‡', './demo.jpg')
                return understanding

            def em_function():
                em = EmebddingModel(config)
                vector = em.get_embedding("æˆ‘ä»¬æ˜¯datawhale")
                return str(vector)

            text_to_audio.click(fn=t2a_function, outputs=gr.Textbox())
            audio_to_text.click(fn=a2t_function, outputs=gr.Textbox())
            generate_image.click(fn=t2i_function, outputs=gr.Image())
            understand_image.click(fn=iu_function, outputs=gr.Textbox())
            get_embedding.click(fn=em_function, outputs=gr.Textbox())

if __name__ == "__main__":
    demo.queue().launch()
